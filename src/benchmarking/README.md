# Benchmarking

We are using the Iguana for the time being to benchmark/stress test our
databases. The jar file can be run by first navigating to the benchmarking
directory and then invoking the `java -jar` command:

```
cd src/benchmarking
java -jar iguana
java -jar iguana-3.3.0.jar config.yml
```

However it's better to use the `npm run` command as it will execute the
pre-hook scripts which auto-populate the queries.txt file, which ensures the
latest query files are being used for the benchmark.

`npm run benchmarkAllSparqlQueries`

## Notes

The `postinstall` script installs the relevant Iguana binaries, and must be
invoked before running the benchmarking process.

# Results

Preliminary results are uploaded to Virtuoso and can be queried like so:

```
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX iprop: <http://iguana-benchmark.eu/properties/>
PREFIX iont: <http://iguana-benchmark.eu/class/>
PREFIX ires: <http://iguana-benchmark.eu/resource/>

SELECT ?taskID ?database ?noq ?QMPH ?noQPH {
    ?suiteID rdf:type iont:Suite .
    ?suiteID iprop:experiment ?expID .
    ?expID iprop:task ?taskID .
    ?taskID iprop:connection ?database .
    ?taskID iprop:NoQ ?noq .
    ?taskID iprop:QMPH ?QMPH .
    ?taskID iprop:NoQPH ?noQPH .
}
```

The `parsedResults.csv` file contains tabulated results which are more human
readable. They are the result of running `src/bin/benchmarking/parseResults.mjs' on the results `.tsv`file. The results`.tsv` file is generated by running
the following query on the relevant SPARQL endpoint:

```
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX iprop: <http://iguana-benchmark.eu/properties/>
PREFIX iont: <http://iguana-benchmark.eu/class/>
PREFIX ires: <http://iguana-benchmark.eu/resource/>

# prefix g (gPredicate, gValue) is for general query - gives us featuers about
# the query regardless of connection.
# prefix c (cPredicate, cValue) is for that particular connections metrics,
# so will give us features regarding to speed, number of failures, etc.
SELECT DISTINCT ?query ?gPredicate ?gValue ?cPredicate ?cValue
WHERE
 {
    # only want queries from connection 1/1 as this is the Virtuoso connection
    <http://iguana-benchmark.eu/resource/1641469666/1/1> iprop:query ?query .
    ?query ?cPredicate ?cValue .
    ?query iprop:queryID ?queryID .
    ?queryID ?gPredicate ?gValue .
}
ORDER BY ?query
```

The query above must be run manually and the resulting file fed into the
parsing script. We have decided not to automate this step as we would like to
move forward with other exploratory work.

## Notes

The queries above are saved at `/src/benchmarking/queries`.

The tabulated results found at `src/benchmarking/parsedResults.csv` contain
boolean columns which describe features relating to the types of queries run during
benchmarking, e.g. `aggregation` and `filter`, when set to 1 mean that query
used both an aggregation and a filter in its query. Due to Iguana being unable
to parse certain queries at runtime, some queries will have a -1 for these
features. You can manually refer to the query ID and `.tsv` file to determine
which of these query features are present.
